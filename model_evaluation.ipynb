{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomedical LLM Evaluation Suite\n",
    "\n",
    "**Project:** Pipeline Optimisation  \n",
    "**Purpose:** Evaluate language models on literature interpretation tasks\n",
    "\n",
    "This notebook uses a two-stage pipeline:\n",
    "1. **Generation**: Evaluation model generates plaintext analysis\n",
    "2. **Parsing**: Instructor extracts structured data from plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (2.9.1)\n",
      "Requirement already satisfied: transformers>=4.35.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (4.57.1)\n",
      "Requirement already satisfied: accelerate>=0.24.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (2.3.5)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
      "Collecting sentencepiece>=0.1.99 (from -r requirements.txt (line 14))\n",
      "  Using cached sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting protobuf>=3.20.0 (from -r requirements.txt (line 15))\n",
      "  Using cached protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting instructor>=1.0.0 (from -r requirements.txt (line 18))\n",
      "  Downloading instructor-1.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic>=2.0.0 (from -r requirements.txt (line 19))\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 22))\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: ipython>=8.12.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 23)) (9.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 25)) (3.10.7)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 26)) (0.13.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./venv/lib/python3.13/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.13/site-packages (from transformers>=4.35.0->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.13/site-packages (from accelerate>=0.24.0->-r requirements.txt (line 4)) (7.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2025.2)\n",
      "Collecting aiohttp<4.0.0,>=3.9.1 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting diskcache>=5.6.3 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docstring-parser<1.0,>=0.16 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting jiter<0.12,>=0.6.1 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading jiter-0.11.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting openai<3.0.0,>=2.0.0 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pre-commit>=4.3.0 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading pre_commit-4.5.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.0 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting rich<15.0.0,>=13.7.0 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10.0.0,>=8.2.3 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting ty>=0.0.1a23 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading ty-0.0.1a27-py3-none-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting typer<1.0.0,>=0.9.0 (from instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->-r requirements.txt (line 19))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.0.0->-r requirements.txt (line 19))\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18)) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.9.1->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (0.28.1)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (3.11)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=2.0.0->instructor>=1.0.0->-r requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->transformers>=4.35.0->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->transformers>=4.35.0->-r requirements.txt (line 3)) (2.5.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<15.0.0,>=13.7.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.13/site-packages (from rich<15.0.0,>=13.7.0->instructor>=1.0.0->-r requirements.txt (line 18)) (2.19.2)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.9.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.9.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: notebook in ./venv/lib/python3.13/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 22)) (7.5.0)\n",
      "Collecting jupyter-console (from jupyter>=1.0.0->-r requirements.txt (line 22))\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nbconvert in ./venv/lib/python3.13/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 22)) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in ./venv/lib/python3.13/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 22)) (7.1.0)\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.13/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 22)) (8.1.8)\n",
      "Requirement already satisfied: jupyterlab in ./venv/lib/python3.13/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 22)) (4.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (3.0.52)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./venv/lib/python3.13/site-packages (from ipython>=8.12.0->-r requirements.txt (line 23)) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=8.12.0->-r requirements.txt (line 23)) (0.2.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 25)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 25)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 25)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 25)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 25)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.13/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 25)) (3.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.13/site-packages (from jedi>=0.18.1->ipython>=8.12.0->-r requirements.txt (line 23)) (0.8.5)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.7.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=8.12.0->-r requirements.txt (line 23)) (0.7.0)\n",
      "Collecting cfgv>=2.0.0 (from pre-commit>=4.3.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting identify>=1.0.0 (from pre-commit>=4.3.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit>=4.3.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting virtualenv>=20.10.0 (from pre-commit>=4.3.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=8.12.0->-r requirements.txt (line 23)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=8.12.0->-r requirements.txt (line 23)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=8.12.0->-r requirements.txt (line 23)) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor>=1.0.0->-r requirements.txt (line 18))\n",
      "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in ./venv/lib/python3.13/site-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor>=1.0.0->-r requirements.txt (line 18)) (4.5.0)\n",
      "Requirement already satisfied: appnope>=0.1.2 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (5.9.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.6.0)\n",
      "Requirement already satisfied: pyzmq>=25 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in ./venv/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 22)) (6.5.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./venv/lib/python3.13/site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 22)) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./venv/lib/python3.13/site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 22)) (3.0.16)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./venv/lib/python3.13/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.0.5)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./venv/lib/python3.13/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./venv/lib/python3.13/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in ./venv/lib/python3.13/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in ./venv/lib/python3.13/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.2.4)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.23.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in ./venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (4.25.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./venv/lib/python3.13/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.29.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (4.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in ./venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (25.10.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.13/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (4.14.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./venv/lib/python3.13/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in ./venv/lib/python3.13/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./venv/lib/python3.13/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./venv/lib/python3.13/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./venv/lib/python3.13/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./venv/lib/python3.13/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.13/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in ./venv/lib/python3.13/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./venv/lib/python3.13/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.21.2)\n",
      "Requirement already satisfied: lark>=1.2.2 in ./venv/lib/python3.13/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./venv/lib/python3.13/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.13/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.13/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 22)) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./venv/lib/python3.13/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 22)) (1.4.0)\n",
      "Using cached sentencepiece-0.2.1-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Using cached protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Downloading instructor-1.13.0-py3-none-any.whl (160 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl (489 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading jiter-0.11.1-cp313-cp313-macosx_11_0_arm64.whl (314 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pre_commit-4.5.0-py2.py3-none-any.whl (226 kB)\n",
      "Downloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n",
      "Downloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\n",
      "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading ty-0.0.1a27-py3-none-macosx_11_0_arm64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: distlib, virtualenv, typing-inspection, ty, tenacity, shellingham, sentencepiece, pydantic-core, protobuf, propcache, nodeenv, multidict, mdurl, jiter, identify, frozenlist, docstring-parser, distro, diskcache, click, cfgv, annotated-types, aiohappyeyeballs, yarl, pydantic, pre-commit, markdown-it-py, aiosignal, rich, openai, aiohttp, typer, jupyter-console, instructor, jupyter\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/35\u001b[0m [jupyter]5;237m━━\u001b[0m \u001b[32m33/35\u001b[0m [instructor]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 cfgv-3.5.0 click-8.3.1 diskcache-5.6.3 distlib-0.4.0 distro-1.9.0 docstring-parser-0.17.0 frozenlist-1.8.0 identify-2.6.15 instructor-1.13.0 jiter-0.11.1 jupyter-1.1.1 jupyter-console-6.6.3 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.7.0 nodeenv-1.9.1 openai-2.8.1 pre-commit-4.5.0 propcache-0.4.1 protobuf-6.33.1 pydantic-2.12.4 pydantic-core-2.41.5 rich-14.2.0 sentencepiece-0.2.1 shellingham-1.5.4 tenacity-9.1.2 ty-0.0.1a27 typer-0.20.0 typing-inspection-0.4.2 virtualenv-20.35.4 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3faf05441dd4bd390c29630a641d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in to HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace (only needed for gated models like Meditron, Llama)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interactive login (recommended for Colab)\n",
    "login()\n",
    "\n",
    "# Option 2: Login with token directly (uncomment and add your token)\n",
    "# login(token='hf_YOUR_TOKEN_HERE')\n",
    "\n",
    "print('✓ Logged in to HuggingFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "Device: CPU\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Instructor for structured output parsing\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "print('✓ Imports successful')\n",
    "print(f\"Device: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Pydantic Models for Structured Outputs\n",
    "# =============================================================================\n",
    "import json as json_module\n",
    "from pydantic import field_validator\n",
    "\n",
    "class RelevanceResult(BaseModel):\n",
    "    \"\"\"Result of relevance assessment.\"\"\"\n",
    "    relevance: Literal[\"relevant\", \"not_relevant\"] = Field(\n",
    "        description=\"Whether the abstract explains how the agent affects the pathway\"\n",
    "    )\n",
    "    rationale: str = Field(\n",
    "        description=\"One sentence explaining the relevance decision\"\n",
    "    )\n",
    "\n",
    "class MechanismResult(BaseModel):\n",
    "    \"\"\"Result of mechanism extraction.\"\"\"\n",
    "    mechanism_summary: str = Field(\n",
    "        description=\"One-sentence description of how the agent affects the pathway\"\n",
    "    )\n",
    "    molecular_components: list[str] = Field(\n",
    "        description=\"List of molecules/genes/proteins mentioned in the mechanism\"\n",
    "    )\n",
    "    direction_of_effect: Literal[\"activation\", \"inhibition\", \"unknown\"] = Field(\n",
    "        description=\"Whether the agent activates or inhibits the pathway\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('molecular_components', mode='before')\n",
    "    @classmethod\n",
    "    def parse_json_string(cls, v):\n",
    "        \"\"\"Handle case where LLM returns JSON string instead of array.\"\"\"\n",
    "        if isinstance(v, str):\n",
    "            try:\n",
    "                parsed = json_module.loads(v)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except json_module.JSONDecodeError:\n",
    "                pass\n",
    "            # If it's a comma-separated string, split it\n",
    "            return [x.strip() for x in v.split(',') if x.strip()]\n",
    "        return v\n",
    "    \n",
    "    @field_validator('direction_of_effect', mode='before')\n",
    "    @classmethod\n",
    "    def parse_enum_dict(cls, v):\n",
    "        \"\"\"Handle case where LLM returns {\"enum\": [\"activation\"]} instead of \"activation\".\"\"\"\n",
    "        if isinstance(v, dict):\n",
    "            # Handle {\"enum\": [\"activation\"]} format\n",
    "            if 'enum' in v and isinstance(v['enum'], list) and len(v['enum']) > 0:\n",
    "                return v['enum'][0]\n",
    "            # Handle other dict formats - try to extract first string value\n",
    "            for val in v.values():\n",
    "                if isinstance(val, str) and val in ('activation', 'inhibition', 'unknown'):\n",
    "                    return val\n",
    "                if isinstance(val, list) and len(val) > 0 and isinstance(val[0], str):\n",
    "                    return val[0]\n",
    "        return v\n",
    "\n",
    "class QualityResult(BaseModel):\n",
    "    \"\"\"Result of evidence quality assessment.\"\"\"\n",
    "    evidence_quality: Literal[\"strong\", \"moderate\", \"weak\", \"insufficient\"] = Field(\n",
    "        description=\"Strength of evidence in the abstract\"\n",
    "    )\n",
    "    justification: str = Field(\n",
    "        description=\"One sentence explaining the quality classification\"\n",
    "    )\n",
    "\n",
    "print('✓ Pydantic models defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded test data:\n",
      "  - Relevance: 50 items\n",
      "  - Mechanism: 50 items\n",
      "  - Quality: 50 items\n",
      "  - Stability: 50 items\n"
     ]
    }
   ],
   "source": [
    "# Load test datasets\n",
    "test_dir = Path('model_tests')\n",
    "\n",
    "with open(test_dir / 'test_relevance.json') as f:\n",
    "    relevance_data = json.load(f)\n",
    "\n",
    "with open(test_dir / 'test_mechanism.json') as f:\n",
    "    mechanism_data = json.load(f)\n",
    "\n",
    "with open(test_dir / 'test_quality.json') as f:\n",
    "    quality_data = json.load(f)\n",
    "\n",
    "with open(test_dir / 'test_stability.json') as f:\n",
    "    stability_data = json.load(f)\n",
    "\n",
    "print(f'✓ Loaded test data:')\n",
    "print(f'  - Relevance: {len(relevance_data)} items')\n",
    "print(f'  - Mechanism: {len(mechanism_data)} items')\n",
    "print(f'  - Quality: {len(quality_data)} items')\n",
    "print(f'  - Stability: {len(stability_data)} items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ModelEvaluator class defined\n"
     ]
    }
   ],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Generates plaintext responses from HuggingFace models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, device='auto'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device if device != 'auto' else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f'Loading model: {model_name}')\n",
    "        print(f'Device: {self.device}')\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map=self.device if self.device == 'cuda' else None\n",
    "            )\n",
    "            self.is_causal = True\n",
    "        except:\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map=self.device if self.device == 'cuda' else None\n",
    "            )\n",
    "            self.is_causal = False\n",
    "        \n",
    "        if self.device == 'cpu':\n",
    "            self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Fix tokenizer pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            if self.tokenizer.unk_token is not None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.unk_token\n",
    "            else:\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        if hasattr(self.model.config, 'max_position_embeddings'):\n",
    "            self.model_max_length = self.model.config.max_position_embeddings\n",
    "        elif hasattr(self.model.config, 'n_positions'):\n",
    "            self.model_max_length = self.model.config.n_positions\n",
    "        else:\n",
    "            self.model_max_length = 1024\n",
    "        \n",
    "        print(f'✓ Model loaded (max length: {self.model_max_length})')\n",
    "    \n",
    "    def generate_response(self, prompt, max_new_tokens=512):\n",
    "        \"\"\"Generate a plaintext response from the model.\"\"\"\n",
    "        safe_input_length = self.model_max_length - max_new_tokens - 10\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=safe_input_length,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    min_new_tokens=20,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "            return response if response else \"No response generated\"\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"[ERROR] Generation failed: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "print('✓ ModelEvaluator class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstructorParser class defined\n"
     ]
    }
   ],
   "source": [
    "class InstructorParser:\n",
    "    \"\"\"Uses Instructor + a parsing LLM to extract structured data from plaintext.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"ollama/llama3.2\"):\n",
    "        \"\"\"\n",
    "        Initialize the parser with a provider.\n",
    "        \n",
    "        Args:\n",
    "            provider: Instructor provider string, e.g.:\n",
    "                - \"ollama/llama3.2\" (local, free)\n",
    "                - \"ollama/mistral\" (local, free)\n",
    "                - \"openai/gpt-4o-mini\" (API, requires OPENAI_API_KEY)\n",
    "        \"\"\"\n",
    "        self.provider = provider\n",
    "        self.client = instructor.from_provider(provider)\n",
    "        print(f'✓ InstructorParser initialized with provider: {provider}')\n",
    "    \n",
    "    def parse(self, text: str, response_model: type[BaseModel], context: str = \"\") -> BaseModel:\n",
    "        \"\"\"Parse plaintext into a structured Pydantic model.\"\"\"\n",
    "        system_prompt = \"\"\"You are a precise data extraction assistant. \n",
    "Extract the requested information from the given text.\n",
    "Only extract what is explicitly stated or clearly implied.\n",
    "If information is missing or unclear, use reasonable defaults.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"Extract structured data from this text.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Text to parse:\n",
    "{text}\n",
    "\n",
    "Extract the information according to the requested schema.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_model=response_model,\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"[PARSE ERROR] {str(e)}\")\n",
    "            if response_model == RelevanceResult:\n",
    "                return RelevanceResult(relevance=\"not_relevant\", rationale=f\"Parse error: {str(e)}\")\n",
    "            elif response_model == MechanismResult:\n",
    "                return MechanismResult(mechanism_summary=f\"Parse error: {str(e)}\", molecular_components=[], direction_of_effect=\"unknown\")\n",
    "            elif response_model == QualityResult:\n",
    "                return QualityResult(evidence_quality=\"insufficient\", justification=f\"Parse error: {str(e)}\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "print('✓ InstructorParser class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "**Recommended models:**\n",
    "- `facebook/galactica-1.3b` - Best for Colab free tier\n",
    "- `facebook/galactica-125m` - Fastest, may struggle\n",
    "- `BioMistral/BioMistral-7B` - Best accuracy, needs Colab Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/galactica-1.3b\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27dd5402dbe4b9db8df9f30eb93f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccfabd42f224bf396962b5c94e8d143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2211cb1e8ae04e0f86d9ef2e768aeaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803daaf64bbc44c29081faf42c8a2de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09768acc0a0d4992926a1fdf55eeb0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e746a03e204912afd356a2a78b7c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583a3d2e1d734e0c820cfab39aa940a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded (max length: 2048)\n",
      "✓ InstructorParser initialized with provider: ollama/llama3.2\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# Model to evaluate (generates plaintext responses)\n",
    "MODEL_NAME = 'facebook/galactica-1.3b'\n",
    "\n",
    "# Parser provider for structured extraction\n",
    "# Options: \"ollama/llama3.2\", \"ollama/mistral\", \"openai/gpt-4o-mini\", etc.\n",
    "PARSER_PROVIDER = \"ollama/llama3.2\"\n",
    "\n",
    "# Initialize evaluator and parser\n",
    "evaluator = ModelEvaluator(MODEL_NAME)\n",
    "parser = InstructorParser(PARSER_PROVIDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test (2 items)\n",
    "\n",
    "Test with 2 items first to verify the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_prompt_template = \"\"\"\n",
    "You are a biomedical expert specializing in molecular mechanisms.\n",
    "\n",
    "Task:\n",
    "Determine whether the abstract explains HOW {agent} affects {pathway} through a *direct or explicitly implied molecular mechanism*.\n",
    "\n",
    "STRICT RELEVANCE CRITERIA:\n",
    "An abstract is **RELEVANT** only if it contains:\n",
    "- A *direct molecular interaction* involving {agent} and components of {pathway}  \n",
    "  (e.g., binding, phosphorylation, gene expression changes, enzyme modulation, signaling events), **OR**\n",
    "- An *explicit implication* of such a mechanism  \n",
    "  (e.g., \"{agent} increases activation of {pathway}-related kinases\",  \n",
    "  \"{agent} suppresses transcription factors that regulate {pathway}\").\n",
    "\n",
    "An abstract is **NOT RELEVANT** if:\n",
    "- {agent} and {pathway} are both mentioned but with **no mechanistic connection**.\n",
    "- No molecular components or processes linking {agent} to {pathway} are described.\n",
    "- Only outcomes, phenotypes, cell effects, or high-level associations are mentioned.\n",
    "- Mechanistic detail is absent, vague, or unrelated to the specified pathway.\n",
    "\n",
    "Hard Constraints:\n",
    "- If the abstract does *not* mention specific molecules, genes, proteins, or signaling components connecting {agent} to {pathway}, output **Not Relevant**.\n",
    "- If the effect is described only as a general phenomenon (e.g., “anti-inflammatory,” “cytotoxic,” “protective”) without molecular detail, output **Not Relevant**.\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Output Requirements:\n",
    "1. Clearly state **“Relevant”** or **“Not Relevant”**.\n",
    "2. Provide a brief, clinical, text-only explanation describing *why* the classification was made, referencing the specific mechanistic elements (or the lack of them).\n",
    "\"\"\"\n",
    "\n",
    "quick_test = relevance_data[:2]\n",
    "results = []\n",
    "\n",
    "for item in tqdm(quick_test, desc='Quick Test'):\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    # Generate plaintext response\n",
    "    start = time.time()\n",
    "    response = evaluator.generate_response(prompt)\n",
    "    gen_time = time.time() - start\n",
    "    \n",
    "    # Parse with Instructor\n",
    "    parse_start = time.time()\n",
    "    parsed = parser.parse(\n",
    "        response, \n",
    "        RelevanceResult,\n",
    "        context=f\"Assessing if abstract about {item['agent']} is relevant to {item['pathway']}\"\n",
    "    )\n",
    "    parse_time = time.time() - parse_start\n",
    "    \n",
    "    predicted = parsed.relevance\n",
    "    expected = item['gold_label'].lower()\n",
    "    \n",
    "    results.append({\n",
    "        'id': item['id'],\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "        'correct': predicted == expected,\n",
    "        'gen_time': gen_time,\n",
    "        'parse_time': parse_time\n",
    "    })\n",
    "    \n",
    "    print(f\"{item['id']}: {predicted} (expected: {expected}) - {'✓' if predicted == expected else '✗'}\")\n",
    "    print(f\"  Gen: {gen_time:.1f}s | Parse: {parse_time:.1f}s\")\n",
    "    print(f\"  Rationale: {parsed.rationale[:100]}...\")\n",
    "    # print(f\"  Raw response: {response[:150]}...\\n\")\n",
    "\n",
    "accuracy = sum(r['correct'] for r in results) / len(results)\n",
    "print(f\"\\nQuick Test Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Avg Gen Time: {sum(r['gen_time'] for r in results)/len(results):.1f}s\")\n",
    "print(f\"Avg Parse Time: {sum(r['parse_time'] for r in results)/len(results):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Relevance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_results = []\n",
    "\n",
    "for item in tqdm(relevance_data, desc='Relevance Test'):\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    response = evaluator.generate_response(prompt)\n",
    "    gen_time = time.time() - start\n",
    "    \n",
    "    parse_start = time.time()\n",
    "    parsed = parser.parse(\n",
    "        response, \n",
    "        RelevanceResult,\n",
    "        context=f\"Assessing if abstract about {item['agent']} is relevant to {item['pathway']}\"\n",
    "    )\n",
    "    parse_time = time.time() - parse_start\n",
    "    \n",
    "    predicted = parsed.relevance\n",
    "    expected = item['gold_label'].lower()\n",
    "    \n",
    "    relevance_results.append({\n",
    "        'id': item['id'],\n",
    "        'agent': item['agent'],\n",
    "        'pathway': item['pathway'],\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "        'correct': predicted == expected,\n",
    "        'rationale': parsed.rationale,\n",
    "        'gen_time': gen_time,\n",
    "        'parse_time': parse_time\n",
    "    })\n",
    "\n",
    "df_relevance = pd.DataFrame(relevance_results)\n",
    "display(df_relevance[['id', 'agent', 'pathway', 'expected', 'predicted', 'correct']])\n",
    "\n",
    "relevance_accuracy = df_relevance['correct'].mean()\n",
    "print(f\"\\nRelevance Accuracy: {relevance_accuracy:.1%}\")\n",
    "print(f\"Correct: {df_relevance['correct'].sum()}/{len(df_relevance)}\")\n",
    "print(f\"Avg Gen Time: {df_relevance['gen_time'].mean():.1f}s\")\n",
    "print(f\"Avg Parse Time: {df_relevance['parse_time'].mean():.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Mechanism Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism_prompt_template = \"\"\"\n",
    "You are a biomedical expert specializing in mechanistic pathway analysis.\n",
    "\n",
    "Task: Extract *only the mechanistic information explicitly stated in the abstract* describing how **{agent}** affects **{pathway}**.\n",
    "\n",
    "Rules:\n",
    "- Use only mechanisms directly reported in the abstract. Do NOT infer, generalize, or assume unstated biology.\n",
    "- Mechanistic detail must be molecular (e.g., protein interactions, gene regulation, signaling events).\n",
    "- You must clearly classify the pathway effect as either **activation** or **inhibition** based solely on explicit text.\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Please provide the following in plain, clinical language:\n",
    "\n",
    "1. **Mechanistic Summary (one sentence):**  \n",
    "   How does {agent} affect {pathway}, based solely on explicit statements?\n",
    "\n",
    "2. **Molecular Components Involved:**  \n",
    "   List all proteins, genes, or molecules described as part of the mechanism linking {agent} to {pathway}.\n",
    "\n",
    "3. **Direction of Effect:**  \n",
    "   Does {agent} *activate* or *inhibit* the pathway?  \n",
    "   (Choose exactly one based on explicit evidence in the abstract.)\n",
    "\"\"\"\n",
    "\n",
    "mechanism_results = []\n",
    "\n",
    "for item in tqdm(mechanism_data, desc='Mechanism Test'):\n",
    "    prompt = mechanism_prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    response = evaluator.generate_response(prompt)\n",
    "    gen_time = time.time() - start\n",
    "    \n",
    "    parse_start = time.time()\n",
    "    parsed = parser.parse(\n",
    "        response, \n",
    "        MechanismResult,\n",
    "        context=f\"Extracting mechanism of {item['agent']} on {item['pathway']}\"\n",
    "    )\n",
    "    parse_time = time.time() - parse_start\n",
    "    \n",
    "    predicted = parsed.direction_of_effect\n",
    "    expected = item['gold_label'].lower()\n",
    "    \n",
    "    mechanism_results.append({\n",
    "        'id': item['id'],\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "        'correct': predicted == expected,\n",
    "        'has_components': len(parsed.molecular_components) > 0,\n",
    "        'num_components': len(parsed.molecular_components),\n",
    "        'summary': parsed.mechanism_summary,\n",
    "        'gen_time': gen_time,\n",
    "        'parse_time': parse_time\n",
    "    })\n",
    "\n",
    "df_mechanism = pd.DataFrame(mechanism_results)\n",
    "display(df_mechanism[['id', 'expected', 'predicted', 'correct', 'num_components']])\n",
    "\n",
    "mechanism_accuracy = df_mechanism['correct'].mean()\n",
    "component_rate = df_mechanism['has_components'].mean()\n",
    "print(f\"\\nMechanism Direction Accuracy: {mechanism_accuracy:.1%}\")\n",
    "print(f\"Correct: {df_mechanism['correct'].sum()}/{len(df_mechanism)}\")\n",
    "print(f\"Component Extraction Rate: {component_rate:.1%}\")\n",
    "print(f\"Avg Components Extracted: {df_mechanism['num_components'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Evidence Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_prompt_template = \"\"\"\n",
    "You are a biomedical expert evaluating research quality.\n",
    "\n",
    "Task: Assess the strength of evidence in this abstract about {agent} and {pathway}.\n",
    "\n",
    "Evidence Quality Scale:\n",
    "- STRONG: Multiple experimental approaches with rigorous controls AND clinical/in vivo validation\n",
    "- MODERATE: Solid experimental support (multiple in vitro assays) but lacking clinical/in vivo validation\n",
    "- WEAK: Minimal experimental data or limited assays\n",
    "- INSUFFICIENT: Unclear methods, anecdotal claims, or very limited evidence\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Please evaluate the evidence quality and explain your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "quality_levels = ['insufficient', 'weak', 'moderate', 'strong']\n",
    "\n",
    "def get_quality_distance(predicted, expected):\n",
    "    try:\n",
    "        pred_idx = quality_levels.index(predicted)\n",
    "        exp_idx = quality_levels.index(expected)\n",
    "        return abs(pred_idx - exp_idx)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "quality_results = []\n",
    "\n",
    "for item in tqdm(quality_data, desc='Quality Test'):\n",
    "    prompt = quality_prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    response = evaluator.generate_response(prompt)\n",
    "    gen_time = time.time() - start\n",
    "    \n",
    "    parse_start = time.time()\n",
    "    parsed = parser.parse(\n",
    "        response, \n",
    "        QualityResult,\n",
    "        context=f\"Evaluating evidence quality for {item['agent']} on {item['pathway']}\"\n",
    "    )\n",
    "    parse_time = time.time() - parse_start\n",
    "    \n",
    "    predicted = parsed.evidence_quality\n",
    "    expected = item['gold_label'].lower()\n",
    "    distance = get_quality_distance(predicted, expected)\n",
    "    \n",
    "    quality_results.append({\n",
    "        'id': item['id'],\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "        'correct': predicted == expected,\n",
    "        'steps_away': distance,\n",
    "        'justification': parsed.justification,\n",
    "        'gen_time': gen_time,\n",
    "        'parse_time': parse_time\n",
    "    })\n",
    "\n",
    "df_quality = pd.DataFrame(quality_results)\n",
    "display(df_quality[['id', 'expected', 'predicted', 'correct', 'steps_away']])\n",
    "\n",
    "quality_accuracy = df_quality['correct'].mean()\n",
    "valid_predictions = df_quality[df_quality['steps_away'] >= 0]\n",
    "within_one_step = (valid_predictions['steps_away'] <= 1).mean() if len(valid_predictions) > 0 else 0\n",
    "\n",
    "print(f\"\\nQuality Accuracy: {quality_accuracy:.1%}\")\n",
    "print(f\"Correct: {df_quality['correct'].sum()}/{len(df_quality)}\")\n",
    "print(f\"Within 1 Step: {within_one_step:.1%}\")\n",
    "print(f\"Avg Steps Away: {valid_predictions['steps_away'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Parsing Stability\n",
    "\n",
    "Run each prompt twice to check if parsing produces consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_results = []\n",
    "\n",
    "for item in tqdm(stability_data, desc='Stability Test'):\n",
    "    prompt = relevance_prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    # Run twice\n",
    "    response1 = evaluator.generate_response(prompt)\n",
    "    parsed1 = parser.parse(\n",
    "        response1, \n",
    "        RelevanceResult,\n",
    "        context=f\"Assessing if abstract about {item['agent']} is relevant to {item['pathway']}\"\n",
    "    )\n",
    "    \n",
    "    response2 = evaluator.generate_response(prompt)\n",
    "    parsed2 = parser.parse(\n",
    "        response2, \n",
    "        RelevanceResult,\n",
    "        context=f\"Assessing if abstract about {item['agent']} is relevant to {item['pathway']}\"\n",
    "    )\n",
    "    \n",
    "    is_stable = parsed1.relevance == parsed2.relevance\n",
    "    \n",
    "    stability_results.append({\n",
    "        'id': item['id'],\n",
    "        'stable': is_stable,\n",
    "        'run1_relevance': parsed1.relevance,\n",
    "        'run2_relevance': parsed2.relevance\n",
    "    })\n",
    "\n",
    "df_stability = pd.DataFrame(stability_results)\n",
    "display(df_stability)\n",
    "\n",
    "stability_rate = df_stability['stable'].mean()\n",
    "print(f\"\\nParsing Stability Rate: {stability_rate:.1%}\")\n",
    "print(f\"Stable: {df_stability['stable'].sum()}/{len(df_stability)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(f\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Evaluation Model:  {MODEL_NAME}\")\n",
    "print(f\"Parser Provider:   {PARSER_PROVIDER}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Test 1 - Relevance Accuracy:    {relevance_accuracy:.1%}\")\n",
    "print(f\"Test 2 - Mechanism Accuracy:    {mechanism_accuracy:.1%}\")\n",
    "print(f\"Test 3 - Evidence Quality:      {quality_accuracy:.1%}\")\n",
    "print(f\"Test 4 - Parsing Stability:     {stability_rate:.1%}\")\n",
    "print(f\"Avg Generation Time:            {df_relevance['gen_time'].mean():.1f}s per item\")\n",
    "print(f\"Avg Parse Time:                 {df_relevance['parse_time'].mean():.1f}s per item\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "accuracies = {\n",
    "    'Relevance': relevance_accuracy,\n",
    "    'Mechanism': mechanism_accuracy,\n",
    "    'Quality': quality_accuracy,\n",
    "    'Stability': stability_rate\n",
    "}\n",
    "colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd']\n",
    "axes[0].bar(accuracies.keys(), accuracies.values(), color=colors)\n",
    "axes[0].set_ylabel('Accuracy / Rate')\n",
    "axes[0].set_title('Performance by Test Type')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "all_gen_times = (list(df_relevance['gen_time']) + list(df_mechanism['gen_time']) + \n",
    "                 list(df_quality['gen_time']))\n",
    "axes[1].hist(all_gen_times, bins=15, color='#1f77b4', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Time (seconds)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Generation Time Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Eval_Model': MODEL_NAME,\n",
    "    'Parser': PARSER_PROVIDER,\n",
    "    'Relevance_Accuracy': f\"{relevance_accuracy:.1%}\",\n",
    "    'Mechanism_Accuracy': f\"{mechanism_accuracy:.1%}\",\n",
    "    'Quality_Accuracy': f\"{quality_accuracy:.1%}\",\n",
    "    'Parsing_Stability': f\"{stability_rate:.1%}\",\n",
    "    'Avg_Gen_Time_s': f\"{df_relevance['gen_time'].mean():.1f}\",\n",
    "    'Avg_Parse_Time_s': f\"{df_relevance['parse_time'].mean():.1f}\"\n",
    "}])\n",
    "\n",
    "display(summary_df)\n",
    "\n",
    "# Download results (works on Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"{MODEL_NAME.replace('/', '_')}_{timestamp}_results.csv\"\n",
    "    summary_df.to_csv(filename, index=False)\n",
    "    files.download(filename)\n",
    "    print(f'\\n✓ Downloaded: {filename}')\n",
    "except:\n",
    "    print('\\n(Not on Colab - results not auto-downloaded)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
