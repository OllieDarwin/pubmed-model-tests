{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomedical LLM Evaluation Suite\n",
    "\n",
    "**Project:** Pipeline Optimisation\n",
    "**Purpose:** Evaluate language models on literature interpretation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (only needed for gated models like Meditron, Llama)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interactive login (recommended for Colab)\n",
    "login()\n",
    "\n",
    "# Option 2: Login with token directly (uncomment and add your token)\n",
    "# login(token='hf_YOUR_TOKEN_HERE')\n",
    "\n",
    "print('✓ Logged in to HuggingFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print('✓ Imports successful')\n",
    "print(f\"Device: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test datasets\n",
    "test_dir = Path('model_tests')\n",
    "\n",
    "with open(test_dir / 'test_relevance.json') as f:\n",
    "    relevance_data = json.load(f)\n",
    "\n",
    "with open(test_dir / 'test_mechanism.json') as f:\n",
    "    mechanism_data = json.load(f)\n",
    "\n",
    "with open(test_dir / 'test_quality.json') as f:\n",
    "    quality_data = json.load(f)\n",
    "\n",
    "with open(test_dir / 'test_stability.json') as f:\n",
    "    stability_data = json.load(f)\n",
    "\n",
    "print(f'✓ Loaded test data:')\n",
    "print(f'  - Relevance: {len(relevance_data)} items')\n",
    "print(f'  - Mechanism: {len(mechanism_data)} items')\n",
    "print(f'  - Quality: {len(quality_data)} items')\n",
    "print(f'  - Stability: {len(stability_data)} items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a test item\n",
    "sample = relevance_data[0]\n",
    "print(f\"ID: {sample['id']}\")\n",
    "print(f\"Agent: {sample['agent']}\")\n",
    "print(f\"Pathway: {sample['pathway']}\")\n",
    "print(f\"Gold Label: {sample['gold_label']}\")\n",
    "print(f\"\\nAbstract (first 200 chars):\\n{sample['abstract'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model_name, device='auto'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device if device != 'auto' else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f'Loading model: {model_name}')\n",
    "        print(f'Device: {self.device}')\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map=self.device if self.device == 'cuda' else None\n",
    "            )\n",
    "            self.is_causal = True\n",
    "        except:\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
    "                device_map=self.device if self.device == 'cuda' else None\n",
    "            )\n",
    "            self.is_causal = False\n",
    "        \n",
    "        if self.device == 'cpu':\n",
    "            self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Fix tokenizer pad token - don't set it to eos_token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            # For models like Mistral/BioMistral, use unk_token or add a new pad token\n",
    "            if self.tokenizer.unk_token is not None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.unk_token\n",
    "            else:\n",
    "                # Add a new pad token\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        if hasattr(self.model.config, 'max_position_embeddings'):\n",
    "            self.model_max_length = self.model.config.max_position_embeddings\n",
    "        elif hasattr(self.model.config, 'n_positions'):\n",
    "            self.model_max_length = self.model.config.n_positions\n",
    "        else:\n",
    "            self.model_max_length = 1024\n",
    "        \n",
    "        print(f'✓ Model loaded (max length: {self.model_max_length})')\n",
    "        print(f'  Tokenizer vocab size: {len(self.tokenizer)}')\n",
    "        print(f'  Tokenizer pad token: {self.tokenizer.pad_token} (ID: {self.tokenizer.pad_token_id})')\n",
    "        print(f'  Tokenizer eos token: {self.tokenizer.eos_token} (ID: {self.tokenizer.eos_token_id})')\n",
    "        print(f'  Model vocab size: {self.model.config.vocab_size}')\n",
    "    \n",
    "    def generate_response(self, prompt, max_new_tokens=256):\n",
    "        safe_input_length = self.model_max_length - max_new_tokens - 10\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=safe_input_length,\n",
    "            padding=False  # Don't pad single sequences\n",
    "        )\n",
    "        \n",
    "        # Move to device and remove unnecessary keys\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "        \n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    min_new_tokens=20,  # Force minimum generation\n",
    "                    temperature=0.7,  # Increased for better sampling\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            # Extract only the generated tokens (not the input)\n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Debug: print first 200 chars if response is empty or very short\n",
    "            if len(response) < 10:\n",
    "                full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"[DEBUG] Short response detected!\")\n",
    "                print(f\"[DEBUG] Input length: {input_length} tokens\")\n",
    "                print(f\"[DEBUG] Output length: {outputs[0].shape[0]} tokens\")\n",
    "                print(f\"[DEBUG] Generated tokens: {len(generated_tokens)}\")\n",
    "                print(f\"[DEBUG] Response: '{response}'\")\n",
    "                print(f\"[DEBUG] Full text (first 300 chars): {full_text[:300]}\")\n",
    "            \n",
    "            return response if response else \"No response generated\"\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_msg = f'{{\"error\": \"Generation failed: {str(e)}\"}}'\n",
    "            print(f\"[ERROR] {error_msg}\")\n",
    "            print(traceback.format_exc())\n",
    "            return error_msg\n",
    "    \n",
    "    def extract_json(self, response):\n",
    "        try:\n",
    "            if '```json' in response:\n",
    "                start = response.find('```json') + 7\n",
    "                end = response.find('```', start)\n",
    "                json_str = response[start:end].strip()\n",
    "            elif '{' in response:\n",
    "                start = response.find('{')\n",
    "                end = response.rfind('}') + 1\n",
    "                json_str = response[start:end]\n",
    "            else:\n",
    "                json_str = response\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            return {'error': f'JSON parsing failed: {str(e)}', 'raw_response': response}\n",
    "\n",
    "print('✓ ModelEvaluator class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "**Recommended models:**\n",
    "- `facebook/galactica-1.3b` - Best for Colab free tier\n",
    "- `facebook/galactica-125m` - Fastest, may struggle\n",
    "- `BioMistral/BioMistral-7B` - Best accuracy, needs Colab Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model here\n",
    "MODEL_NAME = 'facebook/galactica-1.3b'\n",
    "\n",
    "evaluator = ModelEvaluator(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test (2 items)\n",
    "\n",
    "Test with 2 items first to verify the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a biomedical expert analyzing scientific literature.\n",
    "\n",
    "Task: Does this abstract explain how {agent} affects {pathway}?\n",
    "\n",
    "Agent: {agent}\n",
    "Pathway: {pathway}\n",
    "\n",
    "Abstract:\n",
    "{abstract}\n",
    "\n",
    "Instructions:\n",
    "Read the abstract carefully. You must determine if it describes the molecular mechanism connecting THIS SPECIFIC agent to THIS SPECIFIC pathway.\n",
    "\n",
    "Answer \"relevant\" ONLY if the abstract:\n",
    "✓ Names or describes {pathway} explicitly (or its key components)\n",
    "✓ Explains how {agent} molecularly interacts with {pathway}\n",
    "✓ Includes specific proteins/genes showing the connection\n",
    "\n",
    "Answer \"not_relevant\" if:\n",
    "✗ The abstract discusses {agent} but talks about a different pathway\n",
    "✗ The abstract discusses {pathway} but not in relation to {agent}  \n",
    "✗ The agent and pathway are not connected in the text\n",
    "✗ Only clinical outcomes mentioned, no molecular mechanism\n",
    "\n",
    "In your rationale:\n",
    "- State which pathway is actually discussed in the abstract\n",
    "- State whether that matches {pathway}\n",
    "- Be specific about what is or isn't connected\n",
    "\n",
    "ONLY respond in following JSON output format. DO NOT add or remove any values. Anything written inbetween square brackets [] is a placeholder and should be replaced. Do not add anything other than the JSON output in your response\n",
    "\n",
    "Output format:\n",
    "{{\n",
    "  \"relevance\": \"relevant\" or \"not_relevant\",\n",
    "  \"rationale\": \"The abstract discusses [agent]'s effect on [actual pathway mentioned]. This [does/does not] match the specified pathway ({pathway}).\"\n",
    "}}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "quick_test = relevance_data[:2]\n",
    "results = []\n",
    "\n",
    "for item in tqdm(quick_test, desc='Quick Test'):\n",
    "    prompt = prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    response = evaluator.generate_response(prompt)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    parsed = evaluator.extract_json(response)\n",
    "    predicted = parsed.get('relevance', '').lower()\n",
    "    expected = item['gold_label'].lower()\n",
    "    \n",
    "    results.append({\n",
    "        'id': item['id'],\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "        'correct': predicted == expected,\n",
    "        'time': elapsed\n",
    "    })\n",
    "    \n",
    "    print(f\"{item['id']}: {predicted} (expected: {expected}) - {'✓' if predicted == expected else '✗'} [{elapsed:.1f}s]\")\n",
    "    print(f\"Response: {response[:150]}...\\n\")\n",
    "\n",
    "accuracy = sum(r['correct'] for r in results) / len(results)\n",
    "print(f\"\\nQuick Test Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Avg Time: {sum(r['time'] for r in results)/len(results):.1f}s per item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Relevance Assessment (10 items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_results = []\n",
    "\n",
    "for item in tqdm(relevance_data, desc='Relevance Test'):\n",
    "    prompt = prompt_template.format(\n",
    "        agent=item['agent'],\n",
    "        pathway=item['pathway'],\n",
    "        abstract=item['abstract']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    response = evaluator.generate_response(prompt)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    parsed = evaluator.extract_json(response)\n",
    "    predicted = parsed.get('relevance', '').lower()\n",
    "    expected = item['gold_label'].lower()\n",
    "    \n",
    "    relevance_results.append({\n",
    "        'id': item['id'],\n",
    "        'agent': item['agent'],\n",
    "        'pathway': item['pathway'],\n",
    "        'expected': expected,\n",
    "        'predicted': predicted,\n",
    "        'correct': predicted == expected,\n",
    "        'time': elapsed\n",
    "    })\n",
    "\n",
    "df_relevance = pd.DataFrame(relevance_results)\n",
    "display(df_relevance)\n",
    "\n",
    "relevance_accuracy = df_relevance['correct'].mean()\n",
    "print(f\"\\nRelevance Accuracy: {relevance_accuracy:.1%}\")\n",
    "print(f\"Correct: {df_relevance['correct'].sum()}/{len(df_relevance)}\")\n",
    "print(f\"Avg Time: {df_relevance['time'].mean():.1f}s per item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 2: Mechanism Extraction (5 items)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "mechanism_prompt = '''You are a biomedical expert extracting mechanistic information from scientific literature.\n\nTask: Extract the mechanism by which {agent} affects {pathway}.\n\nAgent: {agent}\nPathway: {pathway}\n\nAbstract: {abstract}\n\nProvide extraction as JSON:\n{{\n  \"mechanism_summary\": \"brief description of how the agent affects the pathway\",\n  \"molecular_components\": [\"list\", \"of\", \"key\", \"molecules/proteins/genes\"],\n  \"direction_of_effect\": \"activation\" or \"inhibition\" or \"unknown\"\n}}\n\nJSON Response:'''\n\nmechanism_results = []\n\nfor item in tqdm(mechanism_data, desc='Mechanism Test'):\n    prompt = mechanism_prompt.format(\n        agent=item['agent'],\n        pathway=item['pathway'],\n        abstract=item['abstract']\n    )\n    \n    start = time.time()\n    response = evaluator.generate_response(prompt)\n    elapsed = time.time() - start\n    \n    parsed = evaluator.extract_json(response)\n    predicted = parsed.get('direction_of_effect', '').lower()\n    expected = item['gold_label'].lower()\n    \n    # Check if molecular components were extracted\n    components = parsed.get('molecular_components', [])\n    has_components = len(components) > 0 if isinstance(components, list) else False\n    \n    mechanism_results.append({\n        'id': item['id'],\n        'expected': expected,\n        'predicted': predicted,\n        'correct': predicted == expected,\n        'has_components': has_components,\n        'num_components': len(components) if isinstance(components, list) else 0,\n        'time': elapsed\n    })\n\ndf_mechanism = pd.DataFrame(mechanism_results)\ndisplay(df_mechanism)\n\nmechanism_accuracy = df_mechanism['correct'].mean()\ncomponent_rate = df_mechanism['has_components'].mean()\nprint(f\"\\nMechanism Direction Accuracy: {mechanism_accuracy:.1%}\")\nprint(f\"Correct: {df_mechanism['correct'].sum()}/{len(df_mechanism)}\")\nprint(f\"Component Extraction Rate: {component_rate:.1%}\")\nprint(f\"Avg Components Extracted: {df_mechanism['num_components'].mean():.1f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Test 3: Evidence Quality (5 items)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "quality_prompt = '''You are a biomedical expert evaluating scientific evidence.\n\nTask: Assess the strength of evidence in this abstract.\n\nEvidence Quality Criteria:\n- Strong: Multiple experimental approaches, rigorous controls, clinical validation\n- Moderate: In vitro data with multiple assays\n- Weak: Limited experimental data\n- Insufficient: Minimal data, unclear methods\n\nAgent: {agent}\nPathway: {pathway}\n\nAbstract: {abstract}\n\nProvide assessment as JSON:\n{{\n  \"evidence_quality\": \"strong\" or \"moderate\" or \"weak\" or \"insufficient\",\n  \"justification\": \"brief explanation\"\n}}\n\nJSON Response:'''\n\nquality_results = []\n\nfor item in tqdm(quality_data, desc='Quality Test'):\n    prompt = quality_prompt.format(\n        agent=item['agent'],\n        pathway=item['pathway'],\n        abstract=item['abstract']\n    )\n    \n    start = time.time()\n    response = evaluator.generate_response(prompt)\n    elapsed = time.time() - start\n    \n    parsed = evaluator.extract_json(response)\n    predicted = parsed.get('evidence_quality', '').lower()\n    expected = item['gold_label'].lower()\n    \n    quality_results.append({\n        'id': item['id'],\n        'expected': expected,\n        'predicted': predicted,\n        'correct': predicted == expected,\n        'time': elapsed\n    })\n\ndf_quality = pd.DataFrame(quality_results)\ndisplay(df_quality)\n\nquality_accuracy = df_quality['correct'].mean()\nprint(f\"\\nQuality Accuracy: {quality_accuracy:.1%}\")\nprint(f\"Correct: {df_quality['correct'].sum()}/{len(df_quality)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Test 4: JSON Output Stability (5 items)\n\nRun each prompt twice to check if the model produces consistent JSON structure.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "stability_prompt = '''You are a biomedical expert analyzing scientific literature.\n\nTask: Does this abstract explain how {agent} affects {pathway}?\n\nAgent: {agent}\nPathway: {pathway}\n\nAbstract: {abstract}\n\nRespond ONLY with this exact JSON structure:\n{{\n  \"relevance\": \"relevant\" or \"not_relevant\",\n  \"rationale\": \"brief explanation\"\n}}\n\nJSON Response:'''\n\ndef check_json_stability(parsed1, parsed2):\n    \"\"\"Check if two parsed JSON responses have matching structure.\"\"\"\n    if 'error' in parsed1 or 'error' in parsed2:\n        return False, 'json_parse_failed'\n    \n    # Check field names match\n    keys1 = set(parsed1.keys())\n    keys2 = set(parsed2.keys())\n    if keys1 != keys2:\n        return False, 'field_mismatch'\n    \n    # Check data types match\n    for key in keys1:\n        if type(parsed1[key]) != type(parsed2[key]):\n            return False, 'type_mismatch'\n    \n    return True, 'stable'\n\nstability_results = []\n\nfor item in tqdm(stability_data, desc='Stability Test'):\n    prompt = stability_prompt.format(\n        agent=item['agent'],\n        pathway=item['pathway'],\n        abstract=item['abstract']\n    )\n    \n    # Run twice\n    response1 = evaluator.generate_response(prompt)\n    parsed1 = evaluator.extract_json(response1)\n    \n    response2 = evaluator.generate_response(prompt)\n    parsed2 = evaluator.extract_json(response2)\n    \n    # Check stability\n    is_stable, reason = check_json_stability(parsed1, parsed2)\n    \n    # Check for hallucinated fields (fields not in expected schema)\n    expected_fields = {'relevance', 'rationale'}\n    extra_fields1 = set(parsed1.keys()) - expected_fields - {'error', 'raw_response'}\n    extra_fields2 = set(parsed2.keys()) - expected_fields - {'error', 'raw_response'}\n    has_hallucination = len(extra_fields1) > 0 or len(extra_fields2) > 0\n    \n    stability_results.append({\n        'id': item['id'],\n        'stable': is_stable,\n        'reason': reason,\n        'has_hallucinated_fields': has_hallucination,\n        'run1_fields': list(parsed1.keys()),\n        'run2_fields': list(parsed2.keys())\n    })\n\ndf_stability = pd.DataFrame(stability_results)\ndisplay(df_stability)\n\nstability_rate = df_stability['stable'].mean()\nhallucination_rate = df_stability['has_hallucinated_fields'].mean()\nprint(f\"\\nJSON Stability Rate: {stability_rate:.1%}\")\nprint(f\"Stable: {df_stability['stable'].sum()}/{len(df_stability)}\")\nprint(f\"Hallucination Rate: {hallucination_rate:.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary\nprint(f\"{'='*80}\")\nprint(f\"EVALUATION SUMMARY: {MODEL_NAME}\")\nprint(f\"{'='*80}\")\nprint(f\"Test 1 - Relevance Accuracy:    {relevance_accuracy:.1%}\")\nprint(f\"Test 2 - Mechanism Accuracy:    {mechanism_accuracy:.1%}\")\nprint(f\"Test 3 - Evidence Quality:      {quality_accuracy:.1%}\")\nprint(f\"Test 4 - JSON Stability:        {stability_rate:.1%}\")\nprint(f\"Avg Inference Time:             {df_relevance['time'].mean():.1f}s per item\")\nprint(f\"{'='*80}\")\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Accuracy comparison\naccuracies = {\n    'Relevance': relevance_accuracy,\n    'Mechanism': mechanism_accuracy,\n    'Quality': quality_accuracy,\n    'Stability': stability_rate\n}\ncolors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd']\naxes[0].bar(accuracies.keys(), accuracies.values(), color=colors)\naxes[0].set_ylabel('Accuracy / Rate')\naxes[0].set_title('Performance by Test Type')\naxes[0].set_ylim([0, 1])\naxes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n\n# Time distribution\nall_times = (list(df_relevance['time']) + list(df_mechanism['time']) + \n             list(df_quality['time']))\naxes[1].hist(all_times, bins=15, color='#1f77b4', alpha=0.7, edgecolor='black')\naxes[1].set_xlabel('Time (seconds)')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Inference Time Distribution')\n\nplt.tight_layout()\nplt.show()\n\n# Save results\nsummary_df = pd.DataFrame([{\n    'Model': MODEL_NAME,\n    'Relevance_Accuracy': f\"{relevance_accuracy:.1%}\",\n    'Mechanism_Accuracy': f\"{mechanism_accuracy:.1%}\",\n    'Quality_Accuracy': f\"{quality_accuracy:.1%}\",\n    'JSON_Stability': f\"{stability_rate:.1%}\",\n    'Avg_Time_s': f\"{df_relevance['time'].mean():.1f}\"\n}])\n\ndisplay(summary_df)\n\n# Download results (works on Colab)\ntry:\n    from google.colab import files\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    filename = f\"{MODEL_NAME.replace('/', '_')}_{timestamp}_results.csv\"\n    summary_df.to_csv(filename, index=False)\n    files.download(filename)\n    print(f'\\n✓ Downloaded: {filename}')\nexcept:\n    print('\\n(Not on Colab - results not auto-downloaded)')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}