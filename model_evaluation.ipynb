{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Biomedical LLM Evaluation Suite\n",
        "\n",
        "**Project:** Pipeline Optimisation\n",
        "**Purpose:** Evaluate language models on literature interpretation tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to HuggingFace (only needed for gated models like Meditron, Llama)\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Option 1: Interactive login (recommended for Colab)\n",
        "login()\n",
        "\n",
        "# Option 2: Login with token directly (uncomment and add your token)\n",
        "# login(token='hf_YOUR_TOKEN_HERE')\n",
        "\n",
        "print('✓ Logged in to HuggingFace')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print('✓ Imports successful')\n",
        "print(f\"Device: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test datasets\n",
        "test_dir = Path('model_tests')\n",
        "\n",
        "with open(test_dir / 'test_relevance.json') as f:\n",
        "    relevance_data = json.load(f)\n",
        "\n",
        "with open(test_dir / 'test_mechanism.json') as f:\n",
        "    mechanism_data = json.load(f)\n",
        "\n",
        "with open(test_dir / 'test_quality.json') as f:\n",
        "    quality_data = json.load(f)\n",
        "\n",
        "with open(test_dir / 'test_stability.json') as f:\n",
        "    stability_data = json.load(f)\n",
        "\n",
        "print(f'✓ Loaded test data:')\n",
        "print(f'  - Relevance: {len(relevance_data)} items')\n",
        "print(f'  - Mechanism: {len(mechanism_data)} items')\n",
        "print(f'  - Quality: {len(quality_data)} items')\n",
        "print(f'  - Stability: {len(stability_data)} items')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a test item\n",
        "sample = relevance_data[0]\n",
        "print(f\"ID: {sample['id']}\")\n",
        "print(f\"Agent: {sample['agent']}\")\n",
        "print(f\"Pathway: {sample['pathway']}\")\n",
        "print(f\"Gold Label: {sample['gold_label']}\")\n",
        "print(f\"\\nAbstract (first 200 chars):\\n{sample['abstract'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, model_name, device='auto'):\n",
        "        self.model_name = model_name\n",
        "        self.device = device if device != 'auto' else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        print(f'Loading model: {model_name}')\n",
        "        print(f'Device: {self.device}')\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        \n",
        "        try:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
        "                device_map=self.device if self.device == 'cuda' else None\n",
        "            )\n",
        "            self.is_causal = True\n",
        "        except:\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                model_name,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,\n",
        "                device_map=self.device if self.device == 'cuda' else None\n",
        "            )\n",
        "            self.is_causal = False\n",
        "        \n",
        "        if self.device == 'cpu':\n",
        "            self.model = self.model.to(self.device)\n",
        "        \n",
        "        self.model.eval()\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        if hasattr(self.model.config, 'max_position_embeddings'):\n",
        "            self.model_max_length = self.model.config.max_position_embeddings\n",
        "        elif hasattr(self.model.config, 'n_positions'):\n",
        "            self.model_max_length = self.model.config.n_positions\n",
        "        else:\n",
        "            self.model_max_length = 1024\n",
        "        \n",
        "        print(f'✓ Model loaded (max length: {self.model_max_length})')\n",
        "    \n",
        "    def generate_response(self, prompt, max_new_tokens=256):\n",
        "        safe_input_length = self.model_max_length - max_new_tokens - 10\n",
        "        \n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=safe_input_length)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items() if k != 'token_type_ids'}\n",
        "        \n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    temperature=0.1,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                )\n",
        "            \n",
        "            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            if self.is_causal:\n",
        "                response = full_text[len(prompt):].strip()\n",
        "            else:\n",
        "                response = full_text.strip()\n",
        "            \n",
        "            return response\n",
        "        except Exception as e:\n",
        "            return f'{{\"error\": \"Generation failed: {str(e)}\"}}'\n",
        "    \n",
        "    def extract_json(self, response):\n",
        "        try:\n",
        "            if '```json' in response:\n",
        "                start = response.find('```json') + 7\n",
        "                end = response.find('```', start)\n",
        "                json_str = response[start:end].strip()\n",
        "            elif '{' in response:\n",
        "                start = response.find('{')\n",
        "                end = response.rfind('}') + 1\n",
        "                json_str = response[start:end]\n",
        "            else:\n",
        "                json_str = response\n",
        "            \n",
        "            return json.loads(json_str)\n",
        "        except Exception as e:\n",
        "            return {'error': f'JSON parsing failed: {str(e)}', 'raw_response': response}\n",
        "\n",
        "print('✓ ModelEvaluator class defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model\n",
        "\n",
        "**Recommended models:**\n",
        "- `facebook/galactica-1.3b` - Best for Colab free tier\n",
        "- `facebook/galactica-125m` - Fastest, may struggle\n",
        "- `BioMistral/BioMistral-7B` - Best accuracy, needs Colab Pro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change model here\n",
        "MODEL_NAME = 'facebook/galactica-1.3b'\n",
        "\n",
        "evaluator = ModelEvaluator(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Test (2 items)\n",
        "\n",
        "Test with 2 items first to verify the model works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = '''You are a biomedical expert analyzing scientific literature.\n",
        "\n",
        "Task: Determine if the following PubMed abstract provides mechanistic evidence relating the therapeutic agent to the specified pathway.\n",
        "\n",
        "Agent: {agent}\n",
        "Pathway: {pathway}\n",
        "\n",
        "Abstract: {abstract}\n",
        "\n",
        "Provide your assessment as a JSON object with the following structure:\n",
        "{{\n",
        "  \"relevance\": \"relevant\" or \"not_relevant\",\n",
        "  \"rationale\": \"brief explanation\"\n",
        "}}\n",
        "\n",
        "JSON Response:'''\n",
        "\n",
        "quick_test = relevance_data[:2]\n",
        "results = []\n",
        "\n",
        "for item in tqdm(quick_test, desc='Quick Test'):\n",
        "    prompt = prompt_template.format(\n",
        "        agent=item['agent'],\n",
        "        pathway=item['pathway'],\n",
        "        abstract=item['abstract']\n",
        "    )\n",
        "    \n",
        "    start = time.time()\n",
        "    response = evaluator.generate_response(prompt)\n",
        "    elapsed = time.time() - start\n",
        "    \n",
        "    parsed = evaluator.extract_json(response)\n",
        "    predicted = parsed.get('relevance', '').lower()\n",
        "    expected = item['gold_label'].lower()\n",
        "    \n",
        "    results.append({\n",
        "        'id': item['id'],\n",
        "        'expected': expected,\n",
        "        'predicted': predicted,\n",
        "        'correct': predicted == expected,\n",
        "        'time': elapsed\n",
        "    })\n",
        "    \n",
        "    print(f\"{item['id']}: {predicted} (expected: {expected}) - {'✓' if predicted == expected else '✗'} [{elapsed:.1f}s]\")\n",
        "    print(f\"Response: {response[:150]}...\\n\")\n",
        "\n",
        "accuracy = sum(r['correct'] for r in results) / len(results)\n",
        "print(f\"\\nQuick Test Accuracy: {accuracy:.1%}\")\n",
        "print(f\"Avg Time: {sum(r['time'] for r in results)/len(results):.1f}s per item\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Relevance Assessment (10 items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "relevance_results = []\n",
        "\n",
        "for item in tqdm(relevance_data, desc='Relevance Test'):\n",
        "    prompt = prompt_template.format(\n",
        "        agent=item['agent'],\n",
        "        pathway=item['pathway'],\n",
        "        abstract=item['abstract']\n",
        "    )\n",
        "    \n",
        "    start = time.time()\n",
        "    response = evaluator.generate_response(prompt)\n",
        "    elapsed = time.time() - start\n",
        "    \n",
        "    parsed = evaluator.extract_json(response)\n",
        "    predicted = parsed.get('relevance', '').lower()\n",
        "    expected = item['gold_label'].lower()\n",
        "    \n",
        "    relevance_results.append({\n",
        "        'id': item['id'],\n",
        "        'agent': item['agent'],\n",
        "        'pathway': item['pathway'],\n",
        "        'expected': expected,\n",
        "        'predicted': predicted,\n",
        "        'correct': predicted == expected,\n",
        "        'time': elapsed\n",
        "    })\n",
        "\n",
        "df_relevance = pd.DataFrame(relevance_results)\n",
        "display(df_relevance)\n",
        "\n",
        "relevance_accuracy = df_relevance['correct'].mean()\n",
        "print(f\"\\nRelevance Accuracy: {relevance_accuracy:.1%}\")\n",
        "print(f\"Correct: {df_relevance['correct'].sum()}/{len(df_relevance)}\")\n",
        "print(f\"Avg Time: {df_relevance['time'].mean():.1f}s per item\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Evidence Quality (5 items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quality_prompt = '''You are a biomedical expert evaluating scientific evidence.\n",
        "\n",
        "Task: Assess the strength of evidence in this abstract.\n",
        "\n",
        "Evidence Quality Criteria:\n",
        "- Strong: Multiple experimental approaches, rigorous controls, clinical validation\n",
        "- Moderate: In vitro data with multiple assays\n",
        "- Weak: Limited experimental data\n",
        "- Insufficient: Minimal data, unclear methods\n",
        "\n",
        "Agent: {agent}\n",
        "Pathway: {pathway}\n",
        "\n",
        "Abstract: {abstract}\n",
        "\n",
        "Provide assessment as JSON:\n",
        "{{\n",
        "  \"evidence_quality\": \"strong\" or \"moderate\" or \"weak\" or \"insufficient\",\n",
        "  \"justification\": \"brief explanation\"\n",
        "}}\n",
        "\n",
        "JSON Response:'''\n",
        "\n",
        "quality_results = []\n",
        "\n",
        "for item in tqdm(quality_data, desc='Quality Test'):\n",
        "    prompt = quality_prompt.format(\n",
        "        agent=item['agent'],\n",
        "        pathway=item['pathway'],\n",
        "        abstract=item['abstract']\n",
        "    )\n",
        "    \n",
        "    start = time.time()\n",
        "    response = evaluator.generate_response(prompt)\n",
        "    elapsed = time.time() - start\n",
        "    \n",
        "    parsed = evaluator.extract_json(response)\n",
        "    predicted = parsed.get('evidence_quality', '').lower()\n",
        "    expected = item['gold_label'].lower()\n",
        "    \n",
        "    quality_results.append({\n",
        "        'id': item['id'],\n",
        "        'expected': expected,\n",
        "        'predicted': predicted,\n",
        "        'correct': predicted == expected,\n",
        "        'time': elapsed\n",
        "    })\n",
        "\n",
        "df_quality = pd.DataFrame(quality_results)\n",
        "display(df_quality)\n",
        "\n",
        "quality_accuracy = df_quality['correct'].mean()\n",
        "print(f\"\\nQuality Accuracy: {quality_accuracy:.1%}\")\n",
        "print(f\"Correct: {df_quality['correct'].sum()}/{len(df_quality)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"EVALUATION SUMMARY: {MODEL_NAME}\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Relevance Accuracy:        {relevance_accuracy:.1%}\")\n",
        "print(f\"Evidence Quality Accuracy: {quality_accuracy:.1%}\")\n",
        "print(f\"Avg Inference Time:        {df_relevance['time'].mean():.1f}s per item\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy comparison\n",
        "accuracies = {'Relevance': relevance_accuracy, 'Quality': quality_accuracy}\n",
        "axes[0].bar(accuracies.keys(), accuracies.values(), color=['#1f77b4', '#ff7f0e'])\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Performance by Test Type')\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Time distribution\n",
        "all_times = list(df_relevance['time']) + list(df_quality['time'])\n",
        "axes[1].hist(all_times, bins=15, color='#1f77b4', alpha=0.7, edgecolor='black')\n",
        "axes[1].set_xlabel('Time (seconds)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Inference Time Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': MODEL_NAME,\n",
        "    'Relevance_Accuracy': f\"{relevance_accuracy:.1%}\",\n",
        "    'Quality_Accuracy': f\"{quality_accuracy:.1%}\",\n",
        "    'Avg_Time_s': f\"{df_relevance['time'].mean():.1f}\"\n",
        "}])\n",
        "\n",
        "display(summary_df)\n",
        "\n",
        "# Download results (works on Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f\"{MODEL_NAME.replace('/', '_')}_{timestamp}_results.csv\"\n",
        "    summary_df.to_csv(filename, index=False)\n",
        "    files.download(filename)\n",
        "    print(f'\\n✓ Downloaded: {filename}')\n",
        "except:\n",
        "    print('\\n(Not on Colab - results not auto-downloaded)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
